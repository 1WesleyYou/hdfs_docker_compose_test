# Classification of HDFS Open Issues

## Logging Error

| Issue ID | Version | Trigger | Explain |
|----------|---------|-----------|--------|
| 14945 | 3.1.3 | for a datanode in a pipeline, `PacketResponder` thread encounters an exception, it prints the reason wrongly | file `hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java` line 1491, the warn content should not be `BlockReceiver.run()` but `PacketResponder.run()`|

## Test

| Issue ID | Version | Trigger | Explain |
|----------|---------|-----------|--------|
| 14958 | 3.1.3 | `CommonConfigurationKeysPlublic.NET_TOPOLOGY_IMPL_KEY` is ignored and the test actually uses the default `DFSNetworkTopology`| The flag `DFSConfigKeys.DFS_SUE_NETWORK_TOPOLOGY_KEY` default to true, and thus the `Common...` flag is ignored |
| HDFS-14599 | 3.1.3, 3.2.1, 3.3.0  | `TestDiskBalancerWithFedClusterWithOneNameServiceEmpty` fails   | After HDFS-12487 changed the error message, the test still expects “There are no blocks in the blockPool” but now gets “NextBlock call returned null. No valid block to copy.”, causing the test to fail  [oai_citation:3‡issues.apache.org](https://issues.apache.org/jira/browse/HDFS-14599?page=com.atlassian.jira.plugin.system.issuetabpanels%3Aall-tabpanel&utm_source=chatgpt.com) | Update the expected error string in `TestDiskBalancer#testDiskBalancerWithFedClusterWithOneNameServiceEmpty` to match the new message, or revert HDFS-12487. Fix merged in 3.2.2 and 3.3.0. |

## Erasure Coding Recovery

| Issue ID | Version | Trigger | Description |
|------------|---------|------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|
| 15186 | 3.1.3 | Simultaneous decommission of multiple DataNodes in an EC-enabled cluster | Duplicate `targetIndices` in `StripedReconstructionInfo` cause the EC decoder to treat a source index as a target, producing parity blocks full of zeros |
| HDFS-14920 | 3.0.3, 3.1.3, 3.2.1   | Decommission may hang if one or more DataNodes are out of service during decommission | Flawed calculation in `BlockManager#scheduleReconstruction` subtracts `numReplicas.decommissioning()` (which wrongly includes live replicas) from `additionalReplRequired`, causing it to become zero and skip necessary EC reconstruction tasks  [oai_citation:0‡issues.apache.org](https://issues.apache.org/jira/browse/HDFS-14920) | Correct the logic in `BlockManager#scheduleReconstruction` to exclude live replicas when adjusting `additionalReplRequired`, ensuring reconstruction is scheduled before replication. Patch merged in 3.1.4, 3.2.2, and 3.3.0. |

## Downgrade Compatibility

| Issue ID | Version | Trigger | Description | Resolution |
|------------|---------|---------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 14831 | 3.1.3 | Downgrading from Hadoop 3.2.0 down to 2.7.x | Incompatible changes to the FSImage `StringTable` (commit `8a41edb089fbdedc5e7d9a2aeec63d126afea49f`) prevent a 2.7.x NameNode from reading a 3.x fsimage, causing startup failure :contentReference[oaicite:1]{index=1} | Revert or back‐port the `StringTable` format change (apply commit `8a41edb089fbdedc5e7d9a2aeec63d126afea49f`) so that older NameNodes can read the image (see HDFS-13596), or upgrade to a release that includes this fix :contentReference[oaicite:2]{index=2}. |
| HDFS-14831 | 3.1.3, 3.2.0, 3.3.0    | Downgrading from Hadoop 3.2.0 to 2.7.x        | Incompatible `StringTable` format changes (commit `8a41edb089fbdedc5e7d9a2aeec63d126afea49f`) prevent a 2.7.x NameNode from reading a 3.x FSImage, causing startup failures  [oai_citation:2‡issues.apache.org](https://issues.apache.org/jira/browse/HDFS-14831?focusedCommentId=16926223&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel&utm_source=chatgpt.com) | Back‐port the `StringTable` format change (apply commit `8a41edb089fbdedc5e7d9a2aeec63d126afea49f`) or upgrade to a release that includes the FSImage compatibility fix (see HDFS-13596).                                           |

## HA Checkpoint

| Issue ID   | Version | Trigger                                                                                  | Description                                                                                                                                                                                                                                                | Resolution                                                                                                                                                                                                                                           |
|------------|---------|------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| HDFS-16123 | 3.1.3   | Standby NameNode’s `dfs.namenode.name.dir` disk is full, then space is restored but checkpoint still fails | Standby Checkpointer marks the full storage directory as “failed” and does not attempt to recover it; after space is freed the next checkpoint throws `IOException: No image directories available!`  [oai_citation:1‡issues.apache.org](https://issues.apache.org/jira/browse/HDFS-16123) | Invoke `FSImage.recoverFailedStorage()` at NN startup or on config reload to recover failed dirs; or manually remove the `<name>-current/VERSION.failed` marker and restart the Standby NameNode.                                                        |

## Daemon Resilience

| Issue ID   | Version            | Trigger                                                        | Description                                                                                                                                                                                                                                                                   | Resolution                                                                                                                                                                                                                                   |
|------------|--------------------|----------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| HDFS-16408 | 3.1.3, 3.3.1       | `dfs.namenode.lease-recheck-interval-ms` set to a negative value | Negative value passed to `Thread.sleep()` in `LeaseManager$Monitor.run()`, causing an `IllegalArgumentException` that is caught inside the loop and repeatedly logged, quickly filling logs and consuming disk space  [oai_citation:4‡issues.apache.org](https://issues.apache.org/jira/browse/HDFS-16408)                                        | Validate `dfs.namenode.lease-recheck-interval-ms` on read (e.g. `Preconditions.checkArgument(interval > 0)`) and/or move the `catch(Throwable)` outside the loop so that the thread terminates on unexpected exceptions. Fix in 3.2.4, 3.3.2, 3.4.0. |
